<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions</title>
  <meta name="description"
    content="A solver-free approach for modelling SDE solutions with conditional normalising flows, enabling direct sampling between arbitrary time points.">
  <link rel="canonical" href="https://nkiyohara.github.io/nsf-neurips2025/">
  <meta name="google-site-verification" content="eGSTGsxZa4Nu4AuizmddLZ7aBxPB9lxbAyWBd9RHfyo">
  <meta property="og:title" content="Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions">
  <meta property="og:description"
    content="Solver-free modelling and inference for SDE solutions with conditional normalising flows.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://nkiyohara.github.io/nsf-neurips2025/">
  <meta property="og:image" content="https://nkiyohara.github.io/nsf-neurips2025/assets/images/teaser.jpg">
  <meta property="og:image:alt" content="Teaser illustration for Neural Stochastic Flows">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions">
  <meta name="twitter:description"
    content="Solver-free modelling and inference for SDE solutions with conditional normalising flows.">
  <meta name="twitter:image" content="https://nkiyohara.github.io/nsf-neurips2025/assets/images/teaser.jpg">
  <link rel="icon" type="image/svg+xml" href="favicon.svg">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="css/style.css">
  <!-- Privacy-friendly analytics by Plausible -->
  <script async src="https://plausible.io/js/pa-bt4rw8IO2JnJxjapJaa2f.js"></script>
  <script>
    window.plausible = window.plausible || function () { (plausible.q = plausible.q || []).push(arguments) }, plausible.init = plausible.init || function (i) { plausible.o = i || {} };
    plausible.init();
  </script>
  <script>
    // MathJax configuration - must be set before MathJax loads
    window.MathJax = {
      tex: { inlineMath: [["$", "$"], ["\\(", "\\)"]], displayMath: [["$$", "$$"], ["\\[", "\\]"]] },
      svg: {
        fontCache: "global",
        linebreaks: { automatic: true, width: "container" }
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
  <script src="js/main.js" defer></script>
  <meta name="color-scheme" content="dark light">
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "name": "Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions",
      "headline": "Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions",
      "description": "A solver-free approach for modelling SDE solutions with conditional normalising flows, enabling direct sampling between arbitrary time points.",
      "abstract": "Neural Stochastic Flows (NSFs) learn solver-free transition densities for stochastic differential equations using conditional normalising flows and flow-consistency regularisation. They enable sampling between arbitrary states in a single step, accelerating inference while retaining distributional accuracy across synthetic and real-world datasets.",
      "url": "https://nkiyohara.github.io/nsf-neurips2025/",
      "inLanguage": "en",
      "datePublished": "2025-09-18",
      "image": "https://nkiyohara.github.io/nsf-neurips2025/assets/images/teaser.jpg",
      "author": [
        {
          "@type": "Person",
          "name": "Naoki Kiyohara",
          "affiliation": [
            {
              "@type": "Organization",
              "name": "Imperial College London"
            },
            {
              "@type": "Organization",
              "name": "Canon Inc."
            }
          ]
        },
        {
          "@type": "Person",
          "name": "Edward Johns",
          "affiliation": {
            "@type": "Organization",
            "name": "Imperial College London"
          }
        },
        {
          "@type": "Person",
          "name": "Yingzhen Li",
          "affiliation": {
            "@type": "Organization",
            "name": "Imperial College London"
          }
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "Neural Information Processing Systems"
      }
    }
  </script>
</head>

<body>
  <div class="wrap">
    <header class="hero">
      <p class="venue"><a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/118182" target="_blank"
          rel="noopener">NeurIPS 2025</a></p>
      <h1 class="title">Neural Stochastic Flows:<wbr> Solver-Free Modelling and Inference for SDE Solutions</h1>
      <p class="subtitle">A solver-free approach for modelling SDE solutions with conditional normalising flows, enabling
        direct sampling between arbitrary time points.</p>
      <ul class="authors">
        <li><a href="https://nkiyohara.github.io/" target="_blank" rel="noopener">Naoki
            Kiyohara</a><sup>1,2</sup></li>
        <li><a href="https://www.robot-learning.uk/" target="_blank" rel="noopener">Edward
            Johns</a><sup>1</sup></li>
        <li><a href="https://yingzhenli.net" target="_blank" rel="noopener">Yingzhen
            Li</a><sup>1</sup></li>
      </ul>
      <p class="affil"><sup>1</sup> Imperial College London &nbsp;•&nbsp; <sup>2</sup> Canon Inc.</p>
      <div class="cta">
        <a class="btn primary" href="https://arxiv.org/abs/2510.25769" target="_blank" rel="noopener">
          <svg class="icon icon-file-text" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
            <use href="assets/icons.svg#icon-file-text"></use>
          </svg>
          Paper
        </a>
        <a class="btn ghost" href="https://github.com/nkiyohara/jax_nsf" target="_blank" rel="noopener">
          <svg class="icon icon-github" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
            <use href="assets/icons.svg#icon-github"></use>
          </svg>
          Code
        </a>
        <a class="btn ghost" href="#bibtex">
          <svg class="icon icon-bookmark" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
            <use href="assets/icons.svg#icon-bookmark"></use>
          </svg>
          BibTeX
        </a>
      </div>
    </header>
    <main id="main-content">
    <section id="abstract" class="abstract-section">
      <div class="abstract-card">
        <h2>Abstract</h2>
        <p>Stochastic differential equations (SDEs) are well suited to modelling noisy and/or irregularly-sampled time
          series, which are omnipresent in finance, physics, and machine learning applications. Traditional approaches
          require costly simulation of numerical solvers when sampling between arbitrary time points. <strong>We introduce
          Neural Stochastic Flows (NSFs) and their latent dynamic versions, which learn (latent) SDE transition laws
          directly using conditional normalising flows</strong>, with architectural constraints that preserve properties
          inherited from stochastic flows. <strong>This enables sampling between arbitrary states in a single step</strong>, providing up
          to two orders of magnitude speedup for distant time points. Experiments on synthetic SDE simulations and
          real-world tracking and video data demonstrate that <strong>NSF maintains distributional accuracy comparable to
          numerical approaches</strong> while dramatically reducing computation for arbitrary time-point sampling,
          enabling applications where numerical solvers remain prohibitively expensive.</p>
      </div>
    </section>

    <section id="motivation">
      <div class="panel">
        <h2>Solver-Free Modelling of SDEs</h2>
        <div class="two two-pairs">
          <div>
            <p>Stochastic differential equations (SDEs) capture the noisy dynamics of robotics, finance, climate
              systems, and modern generative models. Real deployments demand forecasts across arbitrary gaps, and
              evaluations must remain fast enough for control or analysis loops.</p>
            <p>Solver-based neural SDEs approximate these transitions by threading hundreds of computational steps, such
              as Euler–Maruyama, so the cost grows linearly with the horizon. This makes long-range or densely queried
              predictions difficult to deploy.</p>
          </div>
          <figure class="video-frame">
            <video controls playsinline preload="metadata"
              data-poster-light="assets/posters/EulerMaruyama-light.jpg"
              data-poster-dark="assets/posters/EulerMaruyama-dark.jpg">
              <source src="assets/videos/dark/sde_animation/720p60/EulerMaruyama.webm" type="video/webm"
                media="(prefers-color-scheme: dark)">
              <source src="assets/videos/light/sde_animation/720p60/EulerMaruyama.webm" type="video/webm"
                media="(prefers-color-scheme: light)">
              <source src="assets/videos/dark/sde_animation/720p60/EulerMaruyama.mp4" type="video/mp4"
                media="(prefers-color-scheme: dark)">
              <source src="assets/videos/light/sde_animation/720p60/EulerMaruyama.mp4" type="video/mp4"
                media="(prefers-color-scheme: light)">
              <source src="assets/videos/light/sde_animation/720p60/EulerMaruyama.mp4" type="video/mp4">
              Your browser does not support HTML5 video.
            </video>
            <figcaption>Numerical SDE solvers require iterative integration steps for long-horizon predictions.
            </figcaption>
          </figure>
          <div>
            <p>To address these computational challenges, we propose <strong>Neural Stochastic Flows (NSFs)</strong> that side-step numerical integration by
              learning the transition density $p_\boldsymbol{\theta}(\boldsymbol{x}_t \mid \boldsymbol{x}_s; \Delta t)$,
              where $\boldsymbol{x}_s$ and $\boldsymbol{x}_t$ are states at times $s$ and $t$ with $s &lt; t$, and $\Delta
                t :=t - s$ is the time gap, in a single pass. The animation shows the solver baseline we replace and
                what NSF models.</p>
          </div>
          <figure class="video-frame">
            <video controls playsinline preload="metadata"
              data-poster-light="assets/posters/EnsemblePathsToDensity-light.jpg"
              data-poster-dark="assets/posters/EnsemblePathsToDensity-dark.jpg">
              <source src="assets/videos/dark/sde_animation/720p60/EnsemblePathsToDensity.webm" type="video/webm"
                media="(prefers-color-scheme: dark)">
              <source src="assets/videos/light/sde_animation/720p60/EnsemblePathsToDensity.webm" type="video/webm"
                media="(prefers-color-scheme: light)">
              <source src="assets/videos/dark/sde_animation/720p60/EnsemblePathsToDensity.mp4" type="video/mp4"
                media="(prefers-color-scheme: dark)">
              <source src="assets/videos/light/sde_animation/720p60/EnsemblePathsToDensity.mp4" type="video/mp4"
                media="(prefers-color-scheme: light)">
              <source src="assets/videos/light/sde_animation/720p60/EnsemblePathsToDensity.mp4" type="video/mp4">
              Your browser does not support HTML5 video.
            </video>
            <figcaption>Neural Stochastic Flows enable direct sampling from conditional densities without iterative solvers.
            </figcaption>
          </figure>
        </div>
      </div>
    </section>

    <section id="approach">
      <div class="panel">
        <h2>Modelling Weak Solutions (Conditional Densities) of SDEs</h2>
        <p>As we describe in the paper, NSFs are designed to fulfil the following inherited properties of <em>stochastic
            flows</em> when viewed as weak solutions. Here, we define the NSF transition density
          $p_\boldsymbol{\theta}(\boldsymbol{x}_t \mid \boldsymbol{x}_s; s, \Delta t)$, where $\boldsymbol{x}_s$ and
          $\boldsymbol{x}_t$ are states at times $s$ and $t$ with $s &lt; t$, and $\Delta t :=t - s$ is the time gap.</p>
            <ol class="flow-list">
              <li>
                <span class="flow-badge">1</span>
                <div>
                  <strong>Independence.</strong>
                  <p>For any $0 \le t_1 \le \dots \le t_n$, the conditionals
                    $p_\boldsymbol{\theta}(\boldsymbol{x}_{t_{k+1}}\mid \boldsymbol{x}_{t_k}; t_k, t_{k+1}-t_k)$ are
                    independent.</p>
                  <p class="table-note">Realised by the <a href="#architecture">architecture</a>: each transition
                    conditions only on $(\boldsymbol{x}_s, s, \Delta t)$ and draws independent Gaussian noise.</p>
                </div>
              </li>
              <li>
                <span class="flow-badge">2</span>
                <div>
                  <strong>Identity.</strong>
                  <p>Evaluating at zero gap recovers the input state: $p_\boldsymbol{\theta}(\boldsymbol{x}_t \mid
                    \boldsymbol{x}_s; s, 0) = \delta(\boldsymbol{x}_t - \boldsymbol{x}_s)$.</p>
                  <p class="table-note">Realised by the <a href="#architecture">architecture</a>: we design the base
                    distribution and coupling layers to collapse to the identity as $\Delta t\to 0$.</p>
                </div>
              </li>
              <li>
                <span class="flow-badge">3</span>
                <div>
                  <strong>Flow property.</strong>
                  <p>For any $0 \le t_i \le t_j \le t_k$, the transition density satisfies the Chapman–Kolmogorov
                    property:</p>
                  <div class="math-block">
                    \[
                    p_\boldsymbol{\theta}(\boldsymbol{x}_{t_k} \mid \boldsymbol{x}_{t_i}; t_i, t_k - t_i) = \int
                    p_\boldsymbol{\theta}(\boldsymbol{x}_{t_k} \mid \boldsymbol{x}_{t_j}; t_j, t_k - t_j)\,
                    p_\boldsymbol{\theta}(\boldsymbol{x}_{t_j} \mid \boldsymbol{x}_{t_i}; t_i, t_j -
                    t_i)\,\mathrm{d}\boldsymbol{x}_{t_j}.
                    \]
                  </div>
                  <p class="table-note">Shaped by the bidirectional flow regulariser introduced in <a
                      href="#flow-consistency">Flow Consistency Regularisation</a>.</p>
                </div>
              </li>
              <li>
                <span class="flow-badge">4</span>
                <div>
                  <strong>Stationarity (for autonomous SDEs).</strong>
                  <p>When the underlying SDE is time-homogeneous, $p_\boldsymbol{\theta}(\boldsymbol{x}_{t_j}\mid
                    \boldsymbol{x}_s; t_i, t_j - t_i)$ depends on the gap only:
                    $p_\boldsymbol{\theta}(\boldsymbol{x}_{t_j}\mid \boldsymbol{x}_s; t_i, t_j -
                    t_i)=p_\boldsymbol{\theta}(\boldsymbol{x}_{t_j+r}\mid \boldsymbol{x}_s; t_i + r, t_j - t_i)$.</p>
                  <p class="table-note">Realised by the <a href="#architecture">architecture</a>: for autonomous SDEs we
                    drop the absolute time $t_i$ from the conditioner so the transition depends only on the gap.</p>
                </div>
              </li>
            </ol>
      </div>
    </section>

    <section id="architecture">
      <div class="panel">
        <h2>Architecture of Neural Stochastic Flows</h2>
        <p>Let $\boldsymbol{c} := (\boldsymbol{x}_{t_i}, \Delta t, t_i)$ denote the conditioning parameters, where $t_i$
          is included only for non-autonomous systems and omitted otherwise, and let $\Delta t := t_j-t_i$. We
          instantiate the sampling procedure of the flow distribution as</p>
        <div class="math-block">
          \[
          \boldsymbol{z} = \underbrace{\boldsymbol{x}_{t_i} + \Delta t \cdot
          \text{MLP}_\mu(\boldsymbol{c};\boldsymbol{\theta}_\mu)}_{\boldsymbol{\mu}(\boldsymbol{c})} +
          \underbrace{\sqrt{\Delta
          t}\cdot\text{MLP}_\sigma(\boldsymbol{c};\boldsymbol{\theta}_\sigma)}_{\boldsymbol{\sigma}(\boldsymbol{c})}
          \odot \,\boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},
          \boldsymbol{I}),
          \]
        </div>
        <div class="math-block">
          \[
          \boldsymbol{x}_{t_j} = \boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{z}, \boldsymbol{c}) =
          \boldsymbol{f}_{L}(\cdot;\boldsymbol{c}, \boldsymbol{\theta}_L) \circ
          \boldsymbol{f}_{L-1}(\cdot;\boldsymbol{c}, \boldsymbol{\theta}_{L-1}) \circ \cdots \circ
          \boldsymbol{f}_{1}(\boldsymbol{z};\boldsymbol{c}, \boldsymbol{\theta}_1).
          \]
        </div>
        <p>Our architecture integrates a parametric Gaussian initialisation with a sequence of bijective
          transformations. The state-dependent Gaussian, centred at $\boldsymbol{\mu}(\boldsymbol{c})$ with noise scale
          $\boldsymbol{\sigma}(\boldsymbol{c})$, follows the similar form to the Euler–Maruyama discretisation with
          drift scaled by $\Delta t$ and diffusion by $\sqrt{\Delta t}$. The subsequent bijective transformations
          $\boldsymbol{f}_1$ through $\boldsymbol{f}_L$ are implemented as conditioned coupling flows whose parameters
          depend on $\boldsymbol{c}$. Each layer splits the state $\boldsymbol{z}$ into two partitions
          $(\boldsymbol{z}_\text{A},\boldsymbol{z}_\text{B})$ and applies an affine update to one partition conditioned
          on the other and on $\boldsymbol{c}$:</p>
        <div class="math-block">
          \[
          \boldsymbol{f}_{ i}(\boldsymbol{z};\boldsymbol{c},\boldsymbol{\theta}_i) =
          \mathrm{Concat}\!\left(\boldsymbol{z}_\text{A},\boldsymbol{z}_\text{B} \odot \exp\!\big(\Delta
          t\,\text{MLP}^{(i)}_{\text{scale}}(\boldsymbol{z}_\text{A},\boldsymbol{c};\boldsymbol{\theta}_\text{scale}^{(i)})\big)
          + \Delta
          t\,\text{MLP}^{(i)}_{\text{shift}}(\boldsymbol{z}_\text{A},\boldsymbol{c};\boldsymbol{\theta}_\text{shift}^{(i)})\right),
          \]
        </div>
        <p>with alternating partitions across layers. The explicit $\Delta t$ factor ensures
          $\boldsymbol{f}_i(\boldsymbol{z};\boldsymbol{c}, \boldsymbol{\theta}_i) = \boldsymbol{z}$ when $\Delta t=0$
          and, combined with the form of the base Gaussian, preserves the identity at zero time gap. Stacking such
          layers yields an expressive diffeomorphism while keeping the Jacobian log-determinant tractable for loss
          computation.</p>

        <h3>Inherited Properties</h3>
        <ul class="checks">
          <li>
            <div class="check-line">
              <span class="check">✓</span>
              <div>
                <strong>Independence.</strong>
                <p>Ensured by the conditional sampling on the initial state $\boldsymbol{x}_{t_i}$ without any overlap
                  between the transitions.</p>
              </div>
            </div>
          </li>
          <li>
            <div class="check-line">
              <span class="check">✓</span>
              <div>
                <strong>Identity.</strong>
                <p>The explicit $\Delta t$ factor in both the base Gaussian and coupling layers ensures
                  $p_\boldsymbol{\theta}(\boldsymbol{x}_t \mid \boldsymbol{x}_s; s, 0) = \delta(\boldsymbol{x}_t -
                  \boldsymbol{x}_s)$.</p>
              </div>
            </div>
          </li>
          <li>
            <div class="check-line">
              <span class="check">✓</span>
              <div>
                <strong>Stationarity (for autonomous SDEs).</strong>
                <p>Obtained by omitting $t_i$ from $\boldsymbol{c}$ when modelling autonomous SDEs.</p>
              </div>
            </div>
          </li>
        </ul>
      </div>
    </section>

    <section id="flow-consistency">
      <div class="panel">
        <h2>Regularisation for Flow Property</h2>
        <div class="two">
          <div>
            <p>To guarantee that recursive application of NSF matches its one-step predictions, we optimise a
              bidirectional KL flow loss that enforces the flow property (Chapman–Kolmogorov property):</p>
            <div class="math-block">
              \[
              p_\boldsymbol{\theta}(\boldsymbol{x}_{t_k} \mid \boldsymbol{x}_{t_i}; t_i, t_k - t_i) = \int
              p_\boldsymbol{\theta}(\boldsymbol{x}_{t_k} \mid \boldsymbol{x}_{t_j}; t_j, t_k - t_j)\,
              p_\boldsymbol{\theta}(\boldsymbol{x}_{t_j} \mid \boldsymbol{x}_{t_i}; t_i, t_j -
              t_i)\,\mathrm{d}\boldsymbol{x}_{t_j}.
              \]
            </div>
            <p>To enforce this property, we minimise the discrepancy between the direct one-step transition (LHS) and
              the two-step transition through an intermediate state (RHS).</p>
          </div>
          <figure class="video-frame">
            <video controls playsinline preload="metadata"
              data-poster-light="assets/posters/ChapmanKolmogorovConsistency-light.jpg"
              data-poster-dark="assets/posters/ChapmanKolmogorovConsistency-dark.jpg">
              <source src="assets/videos/dark/sde_animation/720p60/ChapmanKolmogorovConsistency.webm" type="video/webm"
                media="(prefers-color-scheme: dark)">
              <source src="assets/videos/light/sde_animation/720p60/ChapmanKolmogorovConsistency.webm" type="video/webm"
                media="(prefers-color-scheme: light)">
              <source src="assets/videos/dark/sde_animation/720p60/ChapmanKolmogorovConsistency.mp4" type="video/mp4"
                media="(prefers-color-scheme: dark)">
              <source src="assets/videos/light/sde_animation/720p60/ChapmanKolmogorovConsistency.mp4" type="video/mp4"
                media="(prefers-color-scheme: light)">
              <source src="assets/videos/light/sde_animation/720p60/ChapmanKolmogorovConsistency.mp4" type="video/mp4">
              Your browser does not support HTML5 video.
            </video>
            <figcaption>Flow-consistency loss guides alignment between one-step and two-step transitions.</figcaption>
          </figure>
          <div>
            <p>Several candidates exist such as
              optimal-transport/Wasserstein, Stein, adversarial, and kernel MMD. Among them, we choose KL
              divergences because we can minimise the upper-bounds of the KL divergences in a tractable way, and
              since KL is asymmetric, we use both directions.</p>
            <p>Direct KLs remain intractable due to the marginalisation over the intermediate state
              $\boldsymbol{x}_{t_j}$ on the two-step side. Therefore, we introduce a bridge distribution
              $b_\boldsymbol{\xi}(\boldsymbol{x}_{t_j}\mid\boldsymbol{x}_{t_i},\boldsymbol{x}_{t_k})$ as an auxiliary
              variational distribution (<a href="https://doi.org/10.1007/978-3-540-30499-9_86" target="_blank"
                rel="noopener">Agakov and Barber, 2004</a>; <a href="https://arxiv.org/abs/1511.02386" target="_blank"
                rel="noopener">Ranganath et&nbsp;al., 2016</a>; <a href="https://arxiv.org/abs/1410.6460"
                target="_blank" rel="noopener">Salimans et&nbsp;al., 2015</a>) and optimise variational upper bounds for
              both directions.</p>
            <p>Specifically, for the forward KL (one-step to two-step), we have the upper bound:</p>
            <div class="math-block">
              \[
              \mathcal{L}_{\text{flow, 1-to-2}} = \mathop{\mathbb{E}}_{p_\boldsymbol{\theta}(\boldsymbol{x}_{t_k}\mid
              \boldsymbol{x}_{t_i})b_\boldsymbol{\xi}(\boldsymbol{x}_{t_j}\mid \boldsymbol{x}_{t_i},
              \boldsymbol{x}_{t_k})}\left[\log \frac{p_\boldsymbol{\theta}\left(\boldsymbol{x}_{t_k} \mid
              \boldsymbol{x}_{t_i}\right) \, b_\boldsymbol{\xi}\left(\boldsymbol{x}_{t_j} \mid \boldsymbol{x}_{t_i},
              \boldsymbol{x}_{t_k}\right)}{p_\boldsymbol{\theta}\left(\boldsymbol{x}_{t_k} \mid
              \boldsymbol{x}_{t_j}\right) \, p_\boldsymbol{\theta}\left(\boldsymbol{x}_{t_j} \mid
              \boldsymbol{x}_{t_i}\right)}\right].
              \]
            </div>
            <p>And for the reverse KL (two-step to one-step), we have the upper bound:</p>
            <div class="math-block">
              \[
              \mathcal{L}_{\text{flow, 2-to-1}} = \mathop{\mathbb{E}}_{p_\boldsymbol{\theta}(\boldsymbol{x}_{t_j}\mid
              \boldsymbol{x}_{t_i})p_\boldsymbol{\theta}(\boldsymbol{x}_{t_k} \mid \boldsymbol{x}_{t_j})}\left[\log
              \frac{p_\boldsymbol{\theta}\left(\boldsymbol{x}_{t_j} \mid \boldsymbol{x}_{t_i}\right) \,
              p_\boldsymbol{\theta}\left(\boldsymbol{x}_{t_k} \mid
              \boldsymbol{x}_{t_j}\right)}{b_\boldsymbol{\xi}\left(\boldsymbol{x}_{t_j} \mid \boldsymbol{x}_{t_i},
              \boldsymbol{x}_{t_k}\right) \, p_\boldsymbol{\theta}\left(\boldsymbol{x}_{t_k} \mid
              \boldsymbol{x}_{t_i}\right)}\right].
              \]
            </div>
            <p>The total flow-consistency loss, incorporated as a weighted regulariser alongside the main negative
              log-likelihood objective, is the sum of the two KL divergences:</p>
            <div class="math-block">
              \[
              \mathcal{L}_{\text{flow}} = \mathcal{L}_{\text{flow, 1-to-2}} + \mathcal{L}_{\text{flow, 2-to-1}}.
              \]
            </div>
          </div>
        </div>
      </div>
    </section>

    <section id="latent">
      <div class="panel">
        <h2>Latent Neural Stochastic Flows</h2>
        <p>Irregularly-sampled real-world sequences seldom expose the full system state; instead we observe noisy
          measurements $\boldsymbol{o}_{t_i}$ at times $0 = t_0 &lt; t_1 &lt; \dots &lt; t_T$. To model the hidden
            continuous-time dynamics while remaining solver-free, we introduce the <strong>Latent Neural Stochastic
            Flows (Latent NSFs)</strong> as variational state-space models (VSSMs; <a
              href="https://arxiv.org/abs/1506.02216" target="_blank" rel="noopener">Chung et&nbsp;al., 2015</a>; <a
              href="https://doi.org/10.1609/aaai.v31i1.10779" target="_blank" rel="noopener">Krishnan et&nbsp;al.,
              2017</a>; <a href="https://arxiv.org/abs/1603.06277" target="_blank" rel="noopener">Johnson et&nbsp;al.,
              2016</a>; <a href="https://arxiv.org/abs/1605.06432" target="_blank" rel="noopener">Karl et&nbsp;al.,
              2017</a>) that extend the variational autoencoder framework (<a href="https://arxiv.org/abs/1312.6114"
              target="_blank" rel="noopener">Kingma and Welling, 2014</a>; <a href="https://arxiv.org/abs/1401.4082"
              target="_blank" rel="noopener">Rezende et&nbsp;al., 2014</a>) with NSF transition kernels.</p>
        <div class="image-grid">
          <figure>
            <img src="assets/images/latent/vssm.svg" alt="Graphical model for a standard variational state-space model"
              loading="lazy" class="invert-dark">
            <figcaption>Standard VSSM: discrete-time latent transitions.</figcaption>
          </figure>
          <figure>
            <img src="assets/images/latent/latent_nsf.svg" alt="Graphical model for latent Neural Stochastic Flows"
              loading="lazy" class="invert-dark">
            <figcaption>Latent NSF: solver-free continuous-time transitions parameterised by \(\Delta t\).</figcaption>
          </figure>
          <figure>
            <img src="assets/images/latent/skip_kl.svg" alt="Skip KL objective for latent Neural Stochastic Flows"
              loading="lazy" class="invert-dark">
            <figcaption>Skip-KL objective compares direct transitions with the fully conditioned posterior.</figcaption>
          </figure>
        </div>
        <div class="panel-text">
          <h3>Generative model</h3>
          <p>Let $\boldsymbol{x}_{t_i} \in \mathbb{R}^d$ denote the latent state at time $t_i$, let
            $\boldsymbol{o}_{t_i}$ be the corresponding observation, and define $\Delta t_i := t_i - t_{i-1}$. The joint
            distribution factorises as</p>
          <div class="math-block">
            \[
            \begin{aligned}
            p_{\boldsymbol{\theta},\boldsymbol{\psi}}\bigl(\boldsymbol{x}_{t_{0:T}},\boldsymbol{o}_{t_{0:T}}\bigr)
            &=
            p_{\boldsymbol{\theta}}\!\left(\boldsymbol{x}_{t_0}\right)
            \prod_{i=1}^{T}
            p_{\boldsymbol{\theta}}\!\left(
            \boldsymbol{x}_{t_i}\mid \boldsymbol{x}_{t_{i-1}};
            t_{i-1},\Delta t_i
            \right)
            p_{\boldsymbol{\psi}}\!\left(
            \boldsymbol{o}_{t_i}\mid \boldsymbol{x}_{t_i}
            \right),
            \end{aligned}
            \]
          </div>
          <p class="table-note">Here $p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t_i}\mid \boldsymbol{x}_{t_{i-1}};
            t_{i-1},\Delta t_i)$ is an NSF transition density that admits arbitrary $\Delta t_i$ without numerical
            integration, $p_{\boldsymbol{\psi}}(\boldsymbol{o}_{t_i}\mid \boldsymbol{x}_{t_i})$ is the observation
            decoder, and $p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t_0})$ is an initial prior over the state.</p>

          <h3>Variational posterior</h3>
          <p>For the inference model, we employ a gated recurrent unit encoder (<a
              href="https://doi.org/10.3115/v1/D14-1179" target="_blank" rel="noopener">Cho et&nbsp;al., 2014</a>) that
            processes the observation sequence:</p>
          <div class="math-block">
            \[
            q_{\boldsymbol{\phi}}\!\left(
            \boldsymbol{x}_{t_{0:T}}\mid \boldsymbol{o}_{\le t_T}
            \right)
            =
            \prod_{i=0}^{T}
            \mathcal{N}\!\left(
            \boldsymbol{x}_{t_i}\mid
            \boldsymbol{m}_{t_i},
            \operatorname{diag}\!\left(\boldsymbol{s}_{t_i}^{2}\right)
            \right),
            \]
          </div>
          <div class="math-block">
            \[
            (\boldsymbol{m}_{t_i},\boldsymbol{s}_{t_i})
            =
            \mathrm{GRU}_{\boldsymbol{\phi}}\!\left(
            [\boldsymbol{o}_{t_i},\Delta t_i,t_i],
            \boldsymbol{h}_{t_{i-1}}
            \right),
            \]
          </div>
          <p class="table-note">The hidden state is initialised as $\boldsymbol{h}_{t_{-1}} = \boldsymbol{0}$. Absolute
            time $t_i$ is included only for non-autonomous systems; for autonomous SDEs the encoder conditions on
            $[\boldsymbol{o}_{t_i}, \Delta t_i]$.</p>

          <h3>Learning objective</h3>
          <p>Our goal is to train the generative model
            $p_{\boldsymbol{\theta},\boldsymbol{\psi}}\bigl(\boldsymbol{x}_{t_{0:T}},\boldsymbol{o}_{t_{0:T}}\bigr)$ and
            the inference model $q_{\boldsymbol{\phi}}(\boldsymbol{x}_{t_{0:T}}\mid \boldsymbol{o}_{\le t_T})$. A
            standard choice is the $\beta$-weighted negative ELBO ($\beta$-NELBO) loss (<a
              href="https://openreview.net/forum?id=Sy2fzU9gl" target="_blank" rel="noopener">Higgins et&nbsp;al.,
              2017</a>):</p>
          <div class="math-block">
            \[
            \mathcal{L}_{\beta\text{-NELBO}}
            =
            \sum_{i=0}^{T}
            \Bigl[
            -\,\mathop{\mathbb{E}}_{q_{\boldsymbol{\phi}}}
            \bigl[
            \log
            p_{\boldsymbol{\psi}}(\boldsymbol{o}_{t_i}\mid \boldsymbol{x}_{t_i})
            \bigr]
            +
            \beta
            \mathrm{KL}\bigl(
            q_{\boldsymbol{\phi}}(\boldsymbol{x}_{t_i}\mid \boldsymbol{o}_{\le t_i})
            \,\|\,
            p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t_i}\mid \boldsymbol{x}_{t_{i-1}})
            \bigr)
            \Bigr],
            \]
          </div>
          <p class="table-note">The expectation is over $q_{\boldsymbol{\phi}}(\boldsymbol{x}_{t_i}\mid
            \boldsymbol{o}_{\le t_i})$ and $\mathrm{KL}(\cdot\|\cdot)$ denotes the Kullback–Leibler divergence.</p>
          <p>Focusing only on adjacent time steps can accumulate error over long horizons, so we introduce a skip-ahead
            KL divergence loss (rightmost panel above), akin to latent overshooting (<a
              href="https://arxiv.org/abs/1811.04551" target="_blank" rel="noopener">Hafner et&nbsp;al., 2019</a>):</p>
          <div class="math-block">
            \[
            \mathcal{L}_{\text{skip}} = \sum_{i=0}^{T-\tau}
            \mathop{\mathbb{E}}_{j \sim \mathcal{U}\{i+2,\,i+\tau\}}
            \Bigl[
            \mathrm{KL}\bigl(
            q_{\boldsymbol{\phi}}(\boldsymbol{x}_{t_j}\mid \boldsymbol{o}_{\le t_j})
            \,\|\,
            p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t_j}\mid \boldsymbol{x}_{t_i})
            \bigr)
            \Bigr].
            \]
          </div>
          <p class="table-note">Here $\mathcal{U}\{i+2,\,i+\tau\}$ denotes the discrete uniform distribution over
            indices $\{i+2,\dots,i+\tau\}$ with $\tau \ge 2$. Latent NSF transitions permit direct sampling across
            arbitrary time gaps, making this objective efficient to evaluate.</p>
          <p>Combining the skip-ahead loss with the flow-consistency loss $\mathcal{L}_{\text{flow}}$ yields the total
            loss:</p>
          <div class="math-block">
            \[
            \mathcal{L}_{\text{total}} = \mathcal{L}_{\beta\text{-NELBO}} + \lambda\,\mathcal{L}_{\text{flow}} +
            \beta_{\text{skip}}\,\mathcal{L}_{\text{skip}},
            \]
          </div>
          <p class="table-note">The hyperparameters $\lambda$ and $\beta_{\text{skip}}$ control the strengths of the
            flow-consistency and skip-ahead KL divergence losses, respectively; $\beta$ weights the latent KL in the
            $\beta$-NELBO.</p>
        </div>
      </div>
    </section>

    <section id="results">
      <div class="panel">
        <h2>Results</h2>

        <p>Detailed experimental configurations are documented in the paper.</p>

        <div class="results-toolbar">
          <button class="btn small" type="button" data-toggle-details="open">Expand all</button>
          <button class="btn small" type="button" data-toggle-details="close">Collapse all</button>
        </div>

        <div class="results-accordion">

          <!-- 1) Stochastic Lorenz -->
          <details id="lorenz">
            <summary>
              <span class="summary-left">
                <span class="chev">›</span>
                Stochastic Lorenz Attractor
              </span>
              <span class="summary-right">KL vs Compute • Figures & Table</span>
            </summary>
            <div class="panel-inner">
              <div class="result-block">
                <p>NSF matches the ground-truth attractor while remaining solver-free. Independent one-step samples stay
                  on the correct manifold, unlike solver-based neural SDEs that either diffuse or collapse when rolled
                  forward.</p>
                <div class="image-grid">
                  <figure>
                    <img src="assets/images/lorenz/lorenz_gt.png" alt="Ground-truth Lorenz trajectories"
                      loading="lazy">
                    <figcaption>Ground truth samples</figcaption>
                  </figure>
                  <figure>
                    <img src="assets/images/lorenz/lorenz_latentsde.png" alt="Latent SDE trajectories for Lorenz"
                      loading="lazy">
                    <figcaption>Latent SDE baseline</figcaption>
                  </figure>
                  <figure>
                    <img src="assets/images/lorenz/lorenz_sdematching.png" alt="SDE matching trajectories for Lorenz"
                      loading="lazy">
                    <figcaption>SDE matching baseline</figcaption>
                  </figure>
                  <figure>
                    <img src="assets/images/lorenz/lorenz_nsf1.png" alt="NSF trajectories for Lorenz" loading="lazy">
                    <figcaption>NSF (one-step)</figcaption>
                  </figure>
                </div>
                <table class="result-table">
                  <caption>KL divergence vs. compute on Lorenz (lower is better).</caption>
                  <thead>
                    <tr>
                      <th rowspan="2" scope="col">Method</th>
                      <th colspan="2" scope="colgroup">t = 0.25</th>
                      <th colspan="2" scope="colgroup">t = 0.5</th>
                      <th colspan="2" scope="colgroup">t = 0.75</th>
                      <th colspan="2" scope="colgroup">t = 1.0</th>
                    </tr>
                    <tr>
                      <th scope="col">KL ↓</th>
                      <th scope="col">kFLOPs ↓</th>
                      <th scope="col">KL ↓</th>
                      <th scope="col">kFLOPs ↓</th>
                      <th scope="col">KL ↓</th>
                      <th scope="col">kFLOPs ↓</th>
                      <th scope="col">KL ↓</th>
                      <th scope="col">kFLOPs ↓</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Latent SDE (<a href="https://arxiv.org/abs/2001.01328" target="_blank" rel="noopener">Li
                          et&nbsp;al., 2020</a>)</td>
                      <td>2.1 &plusmn; 0.9</td>
                      <td>959</td>
                      <td>1.8 &plusmn; 0.1</td>
                      <td>1,917</td>
                      <td>0.9 &plusmn; 0.3</td>
                      <td>2,839</td>
                      <td>1.5 &plusmn; 0.5</td>
                      <td>3,760</td>
                    </tr>
                    <tr>
                      <td>Neural LSDE (<a href="https://arxiv.org/abs/2402.14989" target="_blank" rel="noopener">Oh
                          et&nbsp;al., 2024</a>)</td>
                      <td>1.3 &plusmn; 0.4</td>
                      <td>1,712</td>
                      <td>7.2 &plusmn; 1.4</td>
                      <td>3,416</td>
                      <td>74.5 &plusmn; 24.6</td>
                      <td>5,057</td>
                      <td>53.1 &plusmn; 29.3</td>
                      <td>6,699</td>
                    </tr>
                    <tr>
                      <td>Neural GSDE (<a href="https://arxiv.org/abs/2402.14989" target="_blank" rel="noopener">Oh
                          et&nbsp;al., 2024</a>)</td>
                      <td>1.2 &plusmn; 0.4</td>
                      <td>1,925</td>
                      <td>3.9 &plusmn; 0.3</td>
                      <td>3,848</td>
                      <td>20.2 &plusmn; 7.6</td>
                      <td>5,698</td>
                      <td>14.1 &plusmn; 8.4</td>
                      <td>7,548</td>
                    </tr>
                    <tr>
                      <td>Neural LNSDE (<a href="https://arxiv.org/abs/2402.14989" target="_blank" rel="noopener">Oh
                          et&nbsp;al., 2024</a>)</td>
                      <td>1.7 &plusmn; 0.4</td>
                      <td>1,925</td>
                      <td>4.6 &plusmn; 0.7</td>
                      <td>3,848</td>
                      <td>57.3 &plusmn; 16.4</td>
                      <td>5,698</td>
                      <td>44.6 &plusmn; 23.4</td>
                      <td>7,548</td>
                    </tr>
                    <tr class="method-group">
                      <td colspan="9">SDE matching (<a href="https://arxiv.org/abs/2502.02472" target="_blank"
                          rel="noopener">Bartosh et&nbsp;al., 2024</a>)</td>
                    </tr>
                    <tr>
                      <td class="subrow">$\Delta t = 0.0001$</td>
                      <td>4.3 &plusmn; 0.7</td>
                      <td>184,394</td>
                      <td>5.3 &plusmn; 1.0</td>
                      <td>368,787</td>
                      <td>3.4 &plusmn; 0.8</td>
                      <td>553,034</td>
                      <td>3.8 &plusmn; 1.0</td>
                      <td>737,354</td>
                    </tr>
                    <tr>
                      <td class="subrow">$\Delta t = 0.01$</td>
                      <td>6.3 &plusmn; 0.4</td>
                      <td>1,917</td>
                      <td>11.7 &plusmn; 0.5</td>
                      <td>3,834</td>
                      <td>7.9 &plusmn; 0.3</td>
                      <td>5,677</td>
                      <td>6.0 &plusmn; 0.3</td>
                      <td>7,520</td>
                    </tr>
                    <tr class="method-group">
                      <td colspan="9">NSF (ours)</td>
                    </tr>
                    <tr>
                      <td class="subrow">$ \mathcal{H}_{\mathrm{pred}} = 1.0$</td>
                      <td>0.8 &plusmn; 0.7</td>
                      <td>53</td>
                      <td>1.3 &plusmn; 0.1</td>
                      <td>53</td>
                      <td>0.6 &plusmn; 0.3</td>
                      <td>53</td>
                      <td>0.2 &plusmn; 0.6</td>
                      <td>53</td>
                    </tr>
                    <tr>
                      <td class="subrow">$ \mathcal{H}_{\mathrm{pred}} = 0.5$</td>
                      <td>2.4 &plusmn; 1.9</td>
                      <td>53</td>
                      <td>1.3 &plusmn; 0.1</td>
                      <td>53</td>
                      <td>1.0 &plusmn; 0.4</td>
                      <td>105</td>
                      <td>1.7 &plusmn; 1.1</td>
                      <td>105</td>
                    </tr>
                    <tr>
                      <td class="subrow">$ \mathcal{H}_{\mathrm{pred}} = 0.25$</td>
                      <td>1.2 &plusmn; 0.7</td>
                      <td>53</td>
                      <td>1.2 &plusmn; 0.1</td>
                      <td>105</td>
                      <td>0.8 &plusmn; 0.4</td>
                      <td>156</td>
                      <td>1.3 &plusmn; 0.8</td>
                      <td>208</td>
                    </tr>
                  </tbody>
                </table>
                <p class="table-note">Average runtime per 100 samples (JAX): latent SDE 124&ndash;148&nbsp;ms; NSF
                  0.3&nbsp;ms.</p>
              </div>
            </div>
          </details>

          <!-- 2) CMU Mocap -->
          <details id="mocap">
            <summary>
              <span class="summary-left">
                <span class="chev">›</span>
                CMU Motion Capture
              </span>
              <span class="summary-right">Test MSE • Trade-off Plot</span>
            </summary>
            <div class="panel-inner">
              <div class="result-block">
                <p>Latent NSF delivers the strongest extrapolation accuracy while remaining orders of magnitude faster
                  than solver-based latent SDEs. Setup&nbsp;1 evaluates within-horizon forecasting, and Setup&nbsp;2
                  withholds the final third of each sequence during training.</p>
                <table class="result-table">
                  <caption>Test MSE (95% confidence interval) on CMU Motion Capture.</caption>
                  <thead>
                    <tr>
                      <th scope="col">Method</th>
                      <th scope="col">Setup&nbsp;1</th>
                      <th scope="col">Setup&nbsp;2</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>npODE (<a href="https://arxiv.org/abs/1803.04303" target="_blank" rel="noopener">Heinonen
                          et&nbsp;al., 2018</a>)</td>
                      <td>22.96<sup>†</sup></td>
                      <td>&mdash;</td>
                    </tr>
                    <tr>
                      <td>Neural ODE (<a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">Chen
                          et&nbsp;al., 2018</a>)</td>
                      <td>22.49 &plusmn; 0.88<sup>†</sup></td>
                      <td>&mdash;</td>
                    </tr>
                    <tr>
                      <td>ODE2VAE-KL (<a href="https://arxiv.org/abs/1905.10994" target="_blank" rel="noopener">Yildiz
                          et&nbsp;al., 2019</a>)</td>
                      <td>8.09 &plusmn; 1.95<sup>†</sup></td>
                      <td>&mdash;</td>
                    </tr>
                    <tr>
                      <td>Latent ODE (<a href="https://arxiv.org/abs/1907.03907" target="_blank" rel="noopener">Rubanova
                          et&nbsp;al., 2019</a>)</td>
                      <td>5.98 &plusmn; 0.28<sup>*</sup></td>
                      <td>31.62 &plusmn; 0.05<sup>§</sup></td>
                    </tr>
                    <tr>
                      <td>Latent SDE (<a href="https://arxiv.org/abs/2001.01328" target="_blank" rel="noopener">Li
                          et&nbsp;al., 2020</a>)</td>
                      <td>12.91 &plusmn; 2.90<sup>♠</sup></td>
                      <td>9.52 &plusmn; 0.21<sup>§</sup></td>
                    </tr>
                    <tr>
                      <td>Latent Approx SDE (<a href="https://arxiv.org/abs/2110.15739" target="_blank"
                          rel="noopener">Solin et&nbsp;al., 2021</a>)</td>
                      <td>7.55 &plusmn; 0.05<sup>§</sup></td>
                      <td>10.50 &plusmn; 0.86<sup>§</sup></td>
                    </tr>
                    <tr>
                      <td>ARCTA (<a href="https://arxiv.org/abs/2312.10550" target="_blank" rel="noopener">Course
                          et&nbsp;al., 2023</a>)</td>
                      <td>7.62 &plusmn; 0.93<sup>‡</sup></td>
                      <td>9.92 &plusmn; 1.82</td>
                    </tr>
                    <tr>
                      <td>NCDSSM (<a href="https://arxiv.org/abs/2301.11308" target="_blank" rel="noopener">Ansari
                          et&nbsp;al., 2023</a>)</td>
                      <td>5.69 &plusmn; 0.01<sup>§</sup></td>
                      <td>4.74 &plusmn; 0.01<sup>§</sup></td>
                    </tr>
                    <tr>
                      <td>SDE matching (<a href="https://arxiv.org/abs/2502.02472" target="_blank"
                          rel="noopener">Bartosh et&nbsp;al., 2024</a>)</td>
                      <td><strong>5.20 &plusmn; 0.43</strong><sup>♣</sup></td>
                      <td>4.26 &plusmn; 0.35</td>
                    </tr>
                    <tr>
                      <td>Latent NSF (ours)</td>
                      <td>8.62 &plusmn; 0.32</td>
                      <td><strong>3.41 &plusmn; 0.27</strong></td>
                    </tr>
                  </tbody>
                </table>
                <p class="table-note">† reported by Yildiz et&nbsp;al. (2019); * reported by Rubanova et&nbsp;al.
                  (2019); ‡ reported by Course et&nbsp;al. (2023); § reported by Ansari et&nbsp;al. (2023); ♠ our
                  reproduction of Li et&nbsp;al. (2020) <span aria-hidden="true">&mdash;</span> code: <a
                    href="https://github.com/nkiyohara/latent-sde-mocap" target="_blank"
                    rel="noopener">latent-sde-mocap</a>; ♣ our reproduction of Bartosh et&nbsp;al. (2024) <span
                    aria-hidden="true">&mdash;</span> code: <a href="https://github.com/nkiyohara/sde-matching-mocap"
                    target="_blank" rel="noopener">sde-matching-mocap</a>.</p>
                <p class="table-note">Runtime per 100 samples (JAX): latent SDE 75&nbsp;ms vs. Latent NSF 3.5&nbsp;ms.
                </p>
                <figure class="result-figure">
                  <img src="assets/images/mocap_tradeoff.svg" alt="Runtime-accuracy trade-off on CMU Motion Capture"
                    loading="lazy">
                  <figcaption>Latent NSF dominates the runtime/accuracy frontier on both forecasting (left) and
                    extrapolation (right) protocols.</figcaption>
                </figure>
              </div>
            </div>
          </details>

          <!-- 3) Stochastic Moving MNIST -->
          <details id="smmnist">
            <summary>
              <span class="summary-left">
                <span class="chev">›</span>
                Stochastic Moving MNIST
              </span>
              <span class="summary-right">FD Metrics • Grids</span>
            </summary>
            <div class="panel-inner">
              <div class="result-block">
                <p>On video forecasting, Latent NSF preserves appearance quality while modelling stochastic futures.
                  One-step sampling excels on static structure, and recursive rollout remains competitive on dynamics.
                </p>
                <table class="result-table">
                  <caption>Fr&eacute;chet distance (FD) on Stochastic Moving MNIST.</caption>
                  <thead>
                    <tr>
                      <th scope="col">Method</th>
                      <th scope="col">Static FD ↓</th>
                      <th scope="col">Dynamics FD ↓</th>
                      <th scope="col">Frame-wise FD ↓</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Latent SDE</td>
                      <td>2.66 &plusmn; 0.87</td>
                      <td>5.39 &plusmn; 3.10</td>
                      <td><strong>0.58 &plusmn; 0.21</strong></td>
                    </tr>
                    <tr>
                      <td>Latent NSF (recursive)</td>
                      <td>2.36 &plusmn; 0.60</td>
                      <td>7.76 &plusmn; 2.56</td>
                      <td>0.63 &plusmn; 0.17</td>
                    </tr>
                    <tr>
                      <td>Latent NSF (one-step)</td>
                      <td><strong>1.67 &plusmn; 0.47</strong></td>
                      <td>--</td>
                      <td>0.63 &plusmn; 0.15</td>
                    </tr>
                  </tbody>
                </table>
                <p class="table-note">Dynamics FD is undefined for one-step NSF because it predicts the marginal
                  distribution rather than multi-step trajectories.</p>
                <figure class="result-figure small">
                  <img src="assets/images/smmnist_fd.svg" alt="Fréchet distance comparison on Stochastic Moving MNIST"
                    loading="lazy">
                  <figcaption>Latent NSF improves static fidelity while maintaining competitive frame-wise quality;
                    recursive rollout remains close on dynamics.</figcaption>
                </figure>
                <div class="image-grid">
                  <figure>
                    <img src="assets/images/smmnist/ground_truth_grid.png"
                      alt="Ground-truth Stochastic Moving MNIST frames" loading="lazy">
                    <figcaption>Ground-truth future frames</figcaption>
                  </figure>
                  <figure>
                    <img src="assets/images/smmnist/latent_sde_grid.png"
                      alt="Latent SDE Stochastic Moving MNIST predictions" loading="lazy">
                    <figcaption>Latent SDE rollout</figcaption>
                  </figure>
                  <figure>
                    <img src="assets/images/smmnist/latent_nsf_recursive.png"
                      alt="Recursive Latent NSF Stochastic Moving MNIST predictions" loading="lazy">
                    <figcaption>Latent NSF (recursive)</figcaption>
                  </figure>
                  <figure>
                    <img src="assets/images/smmnist/latent_nsf_onestep.png"
                      alt="One-step Latent NSF Stochastic Moving MNIST predictions" loading="lazy">
                    <figcaption>Latent NSF (one-step)</figcaption>
                  </figure>
                </div>
              </div>
            </div>
          </details>

        </div>
      </div>
    </section>

    <section id="related-work">
      <div class="panel">
        <h2>Position of Our Work</h2>
        <p>We categorise prior work in terms of whether learning and/or sampling avoid fine-grained numerical time-stepping, and the class of dynamics targeted, as summarised below.</p>
        <table class="result-table solver-landscape">
          <caption>Solver requirements across continuous-time differential equation models.</caption>
          <thead>
            <tr>
              <th scope="col">Method(s)</th>
              <th scope="col">Target dynamics</th>
              <th scope="col">Solver-free training</th>
              <th scope="col">Solver-free inference</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Neural ODEs (<a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">Chen et&nbsp;al., 2018</a>)</td>
              <td>General ODEs</td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
            </tr>
            <tr>
              <td>Neural flows (<a href="https://arxiv.org/abs/2110.13040" target="_blank" rel="noopener">Bilos et&nbsp;al., 2021</a>)</td>
              <td>General ODEs</td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
            </tr>
            <tr>
              <td>Score-based diffusion via reverse SDEs/PF-ODEs (<a href="https://arxiv.org/abs/2011.13456" target="_blank" rel="noopener">Song et&nbsp;al., 2021</a>; <a href="https://arxiv.org/abs/2210.02747" target="_blank" rel="noopener">Lipman et&nbsp;al., 2023</a>)</td>
              <td>Pre-defined diffusion SDEs/ODEs</td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
            </tr>
            <tr>
              <td>Progressive distillation (<a href="https://openreview.net/forum?id=TIdIXIpzhoI" target="_blank" rel="noopener">Salimans &amp; Ho, 2022</a>)</td>
              <td>Pre-defined diffusion SDEs/ODEs</td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
            </tr>
            <tr>
              <td>Consistency models (<a href="https://arxiv.org/abs/2303.01469" target="_blank" rel="noopener">Song et&nbsp;al., 2023</a>; <a href="https://arxiv.org/abs/2310.02279" target="_blank" rel="noopener">Kim et&nbsp;al., 2024</a>); rectified flows (<a href="https://arxiv.org/abs/2209.03003" target="_blank" rel="noopener">Liu et&nbsp;al., 2023</a>)</td>
              <td>Pre-defined diffusion SDEs/ODEs</td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
            </tr>
            <tr>
              <td>Neural (latent) SDEs (<a href="https://arxiv.org/abs/1905.09883" target="_blank" rel="noopener">Tzen &amp; Raginsky, 2019</a>; <a href="https://arxiv.org/abs/2001.01328" target="_blank" rel="noopener">Li et&nbsp;al., 2020</a>; <a href="https://proceedings.mlr.press/v139/kidger21a.html" target="_blank" rel="noopener">Kidger et&nbsp;al., 2021</a>; <a href="https://openreview.net/forum?id=4VIgNuQ1pY" target="_blank" rel="noopener">Oh et&nbsp;al., 2024</a>; <a href="https://arxiv.org/abs/2110.15739" target="_blank" rel="noopener">Solin et&nbsp;al., 2021</a>)</td>
              <td><span class="target-ito">General It&ocirc; SDEs</span></td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
            </tr>
            <tr>
              <td>ARCTA (<a href="https://openreview.net/forum?id=5yZiP9fZNv" target="_blank" rel="noopener">Course &amp; Nair, 2023</a>); SDE matching (<a href="https://arxiv.org/abs/2502.02472" target="_blank" rel="noopener">Bartosh et&nbsp;al., 2025</a>)</td>
              <td><span class="target-ito">General It&ocirc; SDEs</span></td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
              <td><span class="status status-no" role="img" aria-label="Solver-dependent">✗</span></td>
            </tr>
            <tr>
              <td><strong>Neural Stochastic Flows</strong> (ours)</td>
              <td><span class="target-ito">General It&ocirc; SDEs</span></td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
              <td><span class="status status-yes" role="img" aria-label="Solver-free">✓</span></td>
            </tr>
          </tbody>
        </table>
        <p class="table-note">'Pre-defined diffusion SDEs/ODEs' refers to a boundary-conditioned diffusion process bridging a fixed base distribution and the data distribution utilised in diffusion models.</p>
        <p>NSF unifies the solver-free philosophy of neural flows with the expressive power of neural (latent) SDEs. It learns the Markov transition density as a conditional normalising flow, which satisfies the conditions of weak solutions of SDEs by design and regularisation objectives. Unlike diffusion model-specific accelerations, NSF handles arbitrary It&ocirc; SDEs and extends naturally to latent sequence models. The resulting method removes numerical integration while retaining closed-form likelihoods.</p>
      </div>
    </section>

    <section id="bibtex">
      <div class="panel">
        <h2>BibTeX</h2>
        <div class="bibtex-block">
          <button type="button" class="copy-btn" data-copy-target="#bibtex-entry" aria-label="Copy BibTeX"
            title="Copy BibTeX">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
              stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" focusable="false">
              <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
              <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
            </svg>
            <span class="copy-feedback" aria-hidden="true"></span>
          </button>
          <pre id="bibtex-entry">@inproceedings{kiyohara2025neural,
  title     = {Neural Stochastic Flows: Solver-Free Modelling and Inference for {SDE} Solutions},
  author    = {Naoki Kiyohara and Edward Johns and Yingzhen Li},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2025}
}</pre>
        </div>
      </div>
    </section>

    <div class="lightbox" id="figure-lightbox" aria-hidden="true" role="dialog" aria-label="Expanded figure view">
      <div class="lightbox__backdrop" data-lightbox-dismiss></div>
      <div class="lightbox__content" role="document">
        <button type="button" class="lightbox__close" aria-label="Close expanded figure" data-lightbox-dismiss>
          <svg viewBox="0 0 24 24" aria-hidden="true" focusable="false">
            <path d="M6 6l12 12M18 6L6 18" stroke="currentColor" stroke-width="1.8" stroke-linecap="round"
              stroke-linejoin="round" />
          </svg>
        </button>
        <figure>
          <img src="about:blank" alt="" aria-hidden="true">
          <figcaption></figcaption>
        </figure>
      </div>
    </div>

    </main>
  </div>
</body>

</html>
